---
layout: slide
title: "Method"
---

One of the important steps for safety alignment is to train a binary classifier to identify if a sentence contains harmful content.

We introduce a cost model $c$ to discriminate between safe and unsafe and learn the model using the following pairwise comparison loss:

<img src="{{ site.baseurl }}/assets/img/Picture3.png" >

Itâ€™s worth noting that in the cost model, a response $y$ that is more harmful to the same prompt $x$ will yield a higher cost value. For unsafe responses, the cost value is positive; otherwise, it is negative.
