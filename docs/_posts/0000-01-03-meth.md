---
layout: slide
title: "Method"
---

One of the important steps for safety alignment is to train a binary classifier to identify if a sentence contains harmful content.


Given a dataset \( D=\{x^{i}, y_{\omega}^{i} \succ y_{l}^{i}, s_{\omega}^{i}, s_{l}^{i}\}_{i=1}^{N} \), where \( y_{\omega} \succ y_{l} \) denotes \( y_{l} \) is safer than \( y_{\omega} \), \( s(y)=1 \) if \( y \) is unsafe and \( s(y)=-1 \) otherwise. We can learn a cost model using the following pairwise comparison loss.

\[
\begin{split}
L(c; D)=&-\mathbb{E}_{(x, y_{\omega}, y_{l})\sim D}[\log \sigma(c(x, y_{\omega})-c(x, y_{l}))] \\
&-\mathbb{E}_{(x, y_{\omega}, y_{l},s_{\omega},s_{l})\sim D}[\log \sigma(s_{\omega}c(x, y_{\omega})) \\
&+\log \sigma(s_{l}c(x, y_{l}))] 
\end{split}
\]

Itâ€™s worth noting that in the cost model, a response \( y \) that is more harmful to the same prompt \( x \) will yield a higher cost value. For unsafe responses, the cost value is positive; otherwise, it is negative.
